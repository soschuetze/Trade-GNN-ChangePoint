{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path, PosixPath\n",
    "import json\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import recall_score, precision_score, balanced_accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "from sample import sample_pairs\n",
    "from misc import collate\n",
    "from model import GraphSiamese\n",
    "from embedding import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming root_dir is the path to your root directory\n",
    "root_dir = '../synthetic_experiments/results/synthetic'\n",
    "\n",
    "clique_data = {}\n",
    "cp_times = {}\n",
    "label_data = {}\n",
    "\n",
    "# Walk through all directories and files in root_dir\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # If there's a data.p file in this directory, read it\n",
    "    args_file = os.path.join(dirpath, 'args.json')\n",
    "    if os.path.isfile(args_file):\n",
    "        with open(args_file, 'rb') as f:\n",
    "            arg_data = json.load(f)\n",
    "            clique_size = arg_data['size_clique']\n",
    "\n",
    "    data_file = os.path.join(dirpath, 'data.p')\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            clique_data[clique_size] = data\n",
    "\n",
    "    # If there's a time.json file in this directory, read it\n",
    "    time_file = os.path.join(dirpath, 'time.json')\n",
    "    if os.path.isfile(time_file):\n",
    "        with open(time_file, 'r') as f:\n",
    "            time_data = json.load(f)\n",
    "            cp_times[clique_size] = time_data\n",
    "\n",
    "    label_file = os.path.join(dirpath, 'labels.p')\n",
    "    if os.path.isfile(label_file):\n",
    "        with open(label_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            label_data[clique_size] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/Desktop/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/data/storage.py:450: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 positive and 1000 negative examples\n",
      "500 positive and 500 negative examples\n",
      "500 positive and 500 negative examples\n"
     ]
    }
   ],
   "source": [
    "for s in [70]:\n",
    "    for j, i in enumerate(clique_data[s]):\n",
    "        edge_index = i.edge_index.to(torch.int64)\n",
    "        networkx_graph = to_networkx(i)\n",
    "        adjacency = nx.adjacency_matrix(networkx_graph)\n",
    "        \n",
    "        attributes = np.eye(adjacency.shape[0])\n",
    "        clique_data[s][j].x = attributes\n",
    "    \n",
    "    train = clique_data[s][:1000]\n",
    "    train_labels = label_data[s][:1000]\n",
    "\n",
    "    val = clique_data[s][1000:2000]\n",
    "    val_labels = label_data[s][1000:2000]\n",
    "\n",
    "    test = clique_data[s][2000:]\n",
    "    test_labels = label_data[s][2000:]\n",
    "\n",
    "    graph_pairs_train = sample_pairs(train,train_labels,nsamples=2000)\n",
    "    graph_pairs_val = sample_pairs(val,val_labels,nsamples=1000)\n",
    "    graph_pairs_test = sample_pairs(test,test_labels,nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 70\n",
    "time_test = [t-2000 for t in cp_times[s] if t>=2000]\n",
    "time_train = [t for t in cp_times[s] if t<1000]\n",
    "\n",
    "with open(f'../../results/test_synthetic/test/{s}-data.p', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "\n",
    "with open(f'../../results/test_synthetic/test/{s}-labels.p', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "\n",
    "with open(f'../../results/test_synthetic/test/{s}-time.json', 'w') as f:\n",
    "    json.dump(time_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../graph_pairs_train_{s}.p', 'wb') as f:\n",
    "    pickle.dump(graph_pairs_train, f)\n",
    "with open(f'../../graph_pairs_val_{s}.p', 'wb') as f:\n",
    "    pickle.dump(graph_pairs_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../../graph_pairs_train_{s}.p', 'rb') as f:\n",
    "    graph_pairs_train = pickle.load(f)\n",
    "\n",
    "with open(f'../../graph_pairs_val_{s}.p', 'rb') as f:\n",
    "    graph_pairs_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/Desktop/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "training_data_pairs = DataLoader(graph_pairs_train, batch_size=6, shuffle=True, collate_fn=collate,\n",
    "                               drop_last=True)\n",
    "validation_data_pairs = DataLoader(graph_pairs_val, batch_size=6, shuffle=True, collate_fn=collate,\n",
    "                               drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 30\n",
    "dropout = 0.1\n",
    "input_dim = training_data_pairs.dataset[0][0].x.shape[1]*6\n",
    "embedding = GCN(input_dim=input_dim, type='gcn', hidden_dim=16, layers=3, dropout=dropout)\n",
    "model = GraphSiamese(embedding, 'euclidean', 'topk', 'bce', topk, nlinear=2,\n",
    "                         nhidden=16, dropout=dropout, features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "loss_fn = torch.nn.BCELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = {'train_loss': [], 'train_acc': [], 'train_recall': [], 'train_precision': [],\n",
    "                   'valid_loss': [], 'valid_acc': [], 'valid_precision': [], 'valid_recall': []}\n",
    "\n",
    "best_f1, best_weights, best_loss = 0., None, np.Inf\n",
    "final_metrics = {'loss' : [0.0, 0., 0.], 'accuracy': [0., 0., 0.], 'recall': [0., 0., 0.], 'precision': [0., 0., 0.]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch, Training loss, Valid loss, Valid Acc 0 0.5527933239936829 0.3908514380455017 0.5956790447235107\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 1 0.39355647563934326 0.3225877285003662 0.612139880657196\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 2 0.3523678779602051 0.3239592909812927 0.588477373123169\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 3 0.33206942677497864 0.2096770703792572 0.6013374924659729\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 4 0.32733848690986633 0.27473321557044983 0.6131687164306641\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 5 0.3146028220653534 0.18777506053447723 0.6090534925460815\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 6 0.2913880944252014 0.3033105134963989 0.5943930149078369\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 7 0.291880339384079 0.5142190456390381 0.6203703880310059\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 8 0.26703697443008423 0.16505573689937592 0.6054526567459106\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 9 0.2736589014530182 0.16764608025550842 0.5938786268234253\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 10 0.28180983662605286 0.20118245482444763 0.6193416118621826\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 11 0.2729424238204956 0.18215006589889526 0.5995371341705322\n",
      "Patience counter :  3\n",
      "Epoch, Training loss, Valid loss, Valid Acc 12 0.28067266941070557 0.1565406769514084 0.6147119998931885\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 13 0.27936339378356934 0.1499374508857727 0.6116255521774292\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 14 0.26990827918052673 0.2012653797864914 0.6033951044082642\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 15 0.26999157667160034 0.2615377604961395 0.5676440596580505\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 16 0.26795271039009094 0.12472065538167953 0.6085391640663147\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 17 0.25522705912590027 0.16144898533821106 0.6033951044082642\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 18 0.26871684193611145 0.13981059193611145 0.6036522388458252\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 19 0.26241084933280945 0.1513657569885254 0.6033950448036194\n",
      "Patience counter :  3\n",
      "Epoch, Training loss, Valid loss, Valid Acc 20 0.2627080976963043 0.3591243028640747 0.6026235222816467\n",
      "Patience counter :  4\n",
      "Epoch, Training loss, Valid loss, Valid Acc 21 0.2596666216850281 0.1271490454673767 0.6080247163772583\n",
      "Patience counter :  5\n",
      "Epoch, Training loss, Valid loss, Valid Acc 22 0.2490926831960678 0.19323143362998962 0.6157407164573669\n",
      "Patience counter :  6\n",
      "Epoch, Training loss, Valid loss, Valid Acc 23 0.24303144216537476 0.11005320399999619 0.6108539700508118\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 24 0.2424616515636444 0.20206256210803986 0.605967104434967\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 25 0.24934348464012146 0.12339865416288376 0.6144547462463379\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 26 0.24828842282295227 0.30874744057655334 0.5594136118888855\n",
      "Patience counter :  3\n",
      "Epoch, Training loss, Valid loss, Valid Acc 27 0.23631109297275543 0.15215377509593964 0.6080246567726135\n",
      "Patience counter :  4\n",
      "Epoch, Training loss, Valid loss, Valid Acc 28 0.25220412015914917 0.14697788655757904 0.6036522388458252\n",
      "Patience counter :  5\n",
      "Epoch, Training loss, Valid loss, Valid Acc 29 0.2510443329811096 0.1789407730102539 0.6031378507614136\n",
      "Patience counter :  6\n",
      "Epoch, Training loss, Valid loss, Valid Acc 30 0.23806050419807434 0.10030226409435272 0.6136831641197205\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 31 0.2350834459066391 0.13039685785770416 0.6018518805503845\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 32 0.25931036472320557 0.15224699676036835 0.6082818508148193\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 33 0.24940110743045807 0.11624757945537567 0.59876549243927\n",
      "Patience counter :  3\n",
      "Epoch, Training loss, Valid loss, Valid Acc 34 0.24606509506702423 0.11427193880081177 0.6113682985305786\n",
      "Patience counter :  4\n",
      "Epoch, Training loss, Valid loss, Valid Acc 35 0.24902431666851044 0.36282289028167725 0.5954218506813049\n",
      "Patience counter :  5\n",
      "Epoch, Training loss, Valid loss, Valid Acc 36 0.22814880311489105 0.11269955337047577 0.6075102686882019\n",
      "Patience counter :  6\n",
      "Epoch, Training loss, Valid loss, Valid Acc 37 0.2368995100259781 0.08493009209632874 0.6046810150146484\n",
      "Patience counter :  0\n",
      "Epoch, Training loss, Valid loss, Valid Acc 38 0.22809024155139923 0.14164932072162628 0.6139403581619263\n",
      "Patience counter :  1\n",
      "Epoch, Training loss, Valid loss, Valid Acc 39 0.23020042479038239 0.23757857084274292 0.5707305073738098\n",
      "Patience counter :  2\n",
      "Epoch, Training loss, Valid loss, Valid Acc 40 0.2340327501296997 0.16171979904174805 0.6113682985305786\n",
      "Patience counter :  3\n",
      "Epoch, Training loss, Valid loss, Valid Acc 41 0.2227148413658142 0.10189734399318695 0.6008229851722717\n",
      "Patience counter :  4\n",
      "Epoch, Training loss, Valid loss, Valid Acc 42 0.2305763065814972 0.11475138366222382 0.6005658507347107\n",
      "Patience counter :  5\n",
      "Epoch, Training loss, Valid loss, Valid Acc 43 0.24115827679634094 0.22380264103412628 0.6103395819664001\n",
      "Patience counter :  6\n",
      "Epoch, Training loss, Valid loss, Valid Acc 44 0.24661648273468018 0.1524156630039215 0.6005658507347107\n",
      "Patience counter :  7\n",
      "Epoch, Training loss, Valid loss, Valid Acc 45 0.2165597528219223 0.1303488165140152 0.603909432888031\n",
      "Patience counter :  8\n",
      "Epoch, Training loss, Valid loss, Valid Acc 46 0.23802067339420319 0.11545468866825104 0.5943930149078369\n",
      "Patience counter :  9\n"
     ]
    }
   ],
   "source": [
    "# for early stopping\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "\n",
    "    # training updates\n",
    "    train_loss, train_acc, train_precision, train_recall = [], [], [], []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # minibatch loop\n",
    "    for (graph1, graph2, labels) in training_data_pairs:\n",
    "\n",
    "        graph1, graph2, labels = graph1, graph2, labels\n",
    "\n",
    "        predictions = model(graph1, graph2)\n",
    "        predictions = torch.sigmoid(predictions) # predictions between 0 and 1\n",
    "\n",
    "        loss = loss_fn(predictions, labels.float())\n",
    "\n",
    "        # balanced accuracy score instead of plain accuracy\n",
    "        accuracy = torch.tensor(np.array((predictions.squeeze().cpu().detach() > 0.5) == labels.cpu(),\n",
    "                                            dtype=float).mean().item()).unsqueeze(dim=0)\n",
    "        recall = torch.tensor(\n",
    "            [recall_score(labels.cpu(), (predictions.squeeze().cpu().detach() > 0.5).float(), zero_division=0.)])\n",
    "        precision = torch.tensor(\n",
    "            [precision_score(labels.cpu(), (predictions.squeeze().cpu().detach() > 0.5).float(), zero_division=0.)])\n",
    "\n",
    "        train_loss.append(loss), train_acc.append(accuracy), train_recall.append(recall), train_precision.append(precision), \\\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    logging['train_loss'].append(torch.cat(train_loss).mean().item())\n",
    "    logging['train_acc'].append(torch.cat(train_acc).mean().item())\n",
    "    logging['train_recall'].append(torch.cat(train_recall).mean().item())\n",
    "    logging['train_precision'].append(torch.cat(train_precision).mean().item())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # validation updates\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss, valid_acc, valid_recall, valid_precision = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for (graph1, graph2, labels) in validation_data_pairs:\n",
    "\n",
    "            graph1, graph2 = graph1, graph2\n",
    "\n",
    "            predictions = model(graph1, graph2)\n",
    "\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "\n",
    "            loss = loss_fn(predictions, labels.float())\n",
    "\n",
    "            recall = torch.tensor(\n",
    "                [recall_score(labels.float(), (predictions.squeeze().detach().cpu() > 0.5).float(), zero_division=0.)])\n",
    "            precision = torch.tensor(\n",
    "                [precision_score(labels.float(), (predictions.squeeze().detach().cpu() > 0.5).float(), zero_division=0.)])\n",
    "            accuracy = torch.tensor(np.array((predictions.squeeze().detach().cpu() > 0.5).float() == labels.float(),\n",
    "                                                dtype=float).mean().item()).unsqueeze(dim=0)\n",
    "\n",
    "            valid_loss.append(loss), valid_acc.append(accuracy), valid_recall.append(\n",
    "                recall), valid_precision.append(\n",
    "                precision)\n",
    "\n",
    "        logging['valid_loss'].append(torch.cat(valid_loss).mean().item())\n",
    "        logging['valid_acc'].append(torch.cat(valid_acc).mean().item())\n",
    "        logging['valid_recall'].append(torch.cat(valid_recall).mean().item())\n",
    "        logging['valid_precision'].append(torch.cat(valid_precision).mean().item())\n",
    "\n",
    "        # save best weights\n",
    "        #if logging['valid_f1'][-1] > best_f1 and epoch > 0:\n",
    "        if logging['valid_loss'][-1] < best_loss and epoch > 0:\n",
    "            best_loss = logging['valid_loss'][-1]\n",
    "            #best_f1 = logging['valid_f1'][-1]\n",
    "            final_metrics['loss'][:2] = [logging['train_loss'][-1], logging['valid_loss'][-1]]\n",
    "            final_metrics['accuracy'][:2] = [logging['train_acc'][-1], logging['valid_acc'][-1]]\n",
    "            final_metrics['recall'][:2] = [logging['train_recall'][-1], logging['valid_recall'][-1]]\n",
    "            final_metrics['precision'][:2] = [logging['train_precision'][-1], logging['valid_precision'][-1]]\n",
    "            best_weights = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "    if patience == patience_counter:\n",
    "        break\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        train_acc, train_loss = logging['train_acc'][-1], logging['train_loss'][-1]\n",
    "\n",
    "        valid_acc, valid_loss = logging['valid_acc'][-1], logging['valid_loss'][-1]\n",
    "        print(\"Epoch, Training loss, Valid loss, Valid Acc\", epoch, train_loss, valid_loss,\n",
    "                valid_acc)\n",
    "        print(\"Patience counter : \", patience_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = (f's_{s}_k_{topk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = PosixPath('../../synthetic/trained_models/').expanduser() / model_path\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pd.DataFrame(logging).to_csv(save_dir / 'logging.csv')\n",
    "\n",
    "torch.save(best_weights, save_dir /'model.pt')\n",
    "\n",
    "with open(save_dir / 'results.json', 'w') as fp:\n",
    "    json.dump(final_metrics, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
