{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import datetime as datetime\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.utils.CreateFeatures import CreateFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from src.utils.functions import dist_labels_to_changepoint_labels, dist_labels_to_changepoint_labels_adjusted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.pygcn.batched_model import Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from src.synthetic_experiments.sample import sample_pairs, sample_pairs_in_window\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1962,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/pygcn/all_graphs.pkl\", \"rb\") as f:         \n",
    "    all_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1962, 1967, 1973, 1978, 1981, 1989, 1993, 1996, 2002, 2007, 2012, 2014, 2016]\n",
    "phases = []\n",
    "p = -1\n",
    "for i in range(1962,2019):\n",
    "    if i in crisis_years:\n",
    "        p += 1\n",
    "    phases.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loss_fn, optimiser, training_dataloader):\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    for batch in training_dataloader:\n",
    "        g1, g2, labels = batch\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        predictions = model(g1, g2)\n",
    "        labels = labels.squeeze(1)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        losses.append(loss.detach())\n",
    "        correct += torch.sum((predictions>0.5).long() == labels).item()\n",
    "        total += len(labels)\n",
    "    accuracy = correct / total\n",
    "    return torch.mean(torch.tensor(losses)).item(), accuracy\n",
    "\n",
    "def validate(model, loss_fn, validation_dataloader):\n",
    "    total, correct = 0, 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            g1, g2, labels = batch\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            labels = labels.squeeze(1)\n",
    "            predictions = model(g1, g2)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            losses.append(loss.detach())\n",
    "            correct += torch.sum((predictions>0.5).long() == labels).item()\n",
    "            total += len(labels)\n",
    "    accuracy = correct / total\n",
    "    return torch.mean(torch.tensor(losses)).item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as pyg\n",
    "class MergeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        prog_bar = tqdm(desc=\"Building dataset.\")\n",
    "\n",
    "        self.dataset = []\n",
    "        for p in pairs:\n",
    "\n",
    "            p[0].edge_index = (p[0].edge_index).int()\n",
    "            p[1].edge_index = (p[1].edge_index).int()\n",
    "            self.dataset.append((\n",
    "                p[0],\n",
    "                p[1],\n",
    "                p[2]\n",
    "            ))\n",
    "            prog_bar.update(1)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[pyg.data.Data, pyg.data.Data, torch.Tensor]:\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_dicts/mis_norm.pkl\", \"rb\") as f:\n",
    "    feat_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(years, graphs, feat_dict, dim):\n",
    "\n",
    "    zeros = torch.zeros(dim)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "        new_x = torch.empty(0, dim)\n",
    "        year = years[i]\n",
    "\n",
    "        feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "        for j, country in enumerate(all_nodes):\n",
    "            if j == 0:\n",
    "                new_x = torch.stack([zeros])\n",
    "\n",
    "            elif country in feat_dict_year[\"country_code\"].values:\n",
    "                tensor_before = graphs[i].x[j]\n",
    "                country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "                country_row = country_row.drop(columns = [\"country_code\", \"current_gdp_growth\"])\n",
    "                row_values = country_row.values.tolist()\n",
    "                row_tensor = torch.tensor(row_values)[0]\n",
    "                combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "                new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "            else:\n",
    "                new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "        graphs[i].x = new_x\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/pygcn/all_graphs.pkl\", \"rb\") as f:         \n",
    "    all_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs = add_features(years, all_graphs, feat_dict, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 positive and 168 negative examples\n",
      "27 positive and 18 negative examples\n",
      "33 positive and 32 negative examples\n"
     ]
    }
   ],
   "source": [
    "#For window sampling\n",
    "train_graphs = all_graphs[:34]\n",
    "val_graphs = all_graphs[34:45]\n",
    "test_graphs = all_graphs[45:]\n",
    "\n",
    "labels = dist_labels_to_changepoint_labels(phases)\n",
    "graph_pairs_train = sample_pairs(train_graphs,labels[:34])\n",
    "graph_pairs_val = sample_pairs(val_graphs,labels[34:45])\n",
    "graph_pairs_test = sample_pairs(test_graphs,labels[45:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset.: 561it [00:00, 66125.35it/s]\n",
      "Building dataset.: 55it [00:00, 17909.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric as pyg\n",
    "def collate_fn(batch):\n",
    "    return (\n",
    "        pyg.data.Batch.from_data_list([triple[0] for triple in batch]),\n",
    "        pyg.data.Batch.from_data_list([triple[1] for triple in batch]),\n",
    "        torch.stack([triple[2] for triple in batch])\n",
    "    )\n",
    "\n",
    "training_data = MergeDataset(graph_pairs_train)\n",
    "validation_data = MergeDataset(graph_pairs_val)\n",
    "training_dataloader = DataLoader(training_data, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "validation_dataloader =  DataLoader(validation_data, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.6207\t0.66\t0.6441\t0.65\n",
      "1\t0.606\t0.69\t0.6428\t0.65\n",
      "2\t0.6066\t0.68\t0.6531\t0.65\n",
      "3\t0.6006\t0.68\t0.6581\t0.58\n",
      "4\t0.6008\t0.68\t0.6416\t0.6\n",
      "5\t0.5951\t0.64\t0.6505\t0.56\n",
      "6\t0.5872\t0.64\t0.6456\t0.56\n",
      "7\t0.5804\t0.67\t0.65\t0.56\n",
      "8\t0.582\t0.68\t0.6596\t0.56\n",
      "9\t0.5721\t0.67\t0.6355\t0.58\n",
      "10\t0.564\t0.7\t0.6203\t0.64\n",
      "11\t0.5321\t0.77\t0.6679\t0.51\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(12):\n",
    "    train_loss, train_accuracy = train_epoch(model, loss_fn, optimiser, training_dataloader)\n",
    "    valid_loss, valid_accuracy = validate(model, loss_fn, validation_dataloader)\n",
    "    print(\n",
    "        epoch, \n",
    "        round(train_loss, 4), \n",
    "        round(train_accuracy, 2),\n",
    "        round(valid_loss, 4), \n",
    "        round(valid_accuracy, 2),\n",
    "        sep='\\t'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
